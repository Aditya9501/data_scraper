from selenium import webdriver
import pandas as pd
import time
from time import sleep
from selenium.webdriver.common.by import By
from selenium.webdriver import ActionChains
from selenium.webdriver.support.wait import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

def get_job_without_login(jobdesgn,yrsexp,url="https://www.naukri.com"):

    driver = webdriver.Chrome()
    driver.get(url)
    driver.maximize_window()

    wait = WebDriverWait(driver,10)
    chain = ActionChains(driver)

    # Input the job you wish to search for
    search = (By.XPATH,"(//input[contains(@placeholder,'Enter skills / designations / companies')])[1]")
    wait.until(EC.element_to_be_clickable(search)).send_keys(jobdesgn)

    # Click on Search
    search_click = (By.CSS_SELECTOR, ".qsbSubmit")
    wait.until(EC.element_to_be_clickable(search_click)).click()

    # Filter the results using given yrs of experience
    wait.until(EC.element_to_be_clickable((By.XPATH,"//div[@class='inside']")))
    dragger=driver.find_element(By.XPATH,"//div[@class='inside']")
    chain.move_to_element(dragger).click_and_hold().pause(1).drag_and_drop_by_offset(dragger,(((yrsexp)*7)-210),0).perform()

    # Display number of all the jobs 
    wait.until(EC.visibility_of_element_located((By.XPATH, "/html[1]/body[1]/div[1]/div[4]/div[1]/div[1]/section[2]/div[1]/div[1]/span[1]")))
    total_listing = driver.find_element(By.XPATH, "/html[1]/body[1]/div[1]/div[4]/div[1]/div[1]/section[2]/div[1]/div[1]/span[1]").text
    pages = int(int(total_listing.split(" ")[-1])/(int(total_listing.split(' ')[2])))

    print(f'Total Job postings are {total_listing.split(" ")[-1]}')

    job_link = []
    i=0
    while i<pages:
        
        jobs_wait = (By.XPATH,"//article")
        wait.until(EC.visibility_of_all_elements_located(jobs_wait))
        
        jobs = driver.find_elements(By.XPATH,"//article")

        for j in range(len(jobs)):
            if jobdesgn.lower() in driver.find_element(By.XPATH, f'//article[{j+1}]/div[1]/div[1]/a[1]').text.lower():
                job_link.append(driver.find_element(By.XPATH, f'//article[{j+1}]/div[1]/div[1]/a[1]').get_attribute('href'))

        driver.execute_script("window.scrollBy(0,4800)","")
        try:
            wait.until(EC.element_to_be_clickable((By.XPATH,"//a[@class='fright fs14 btn-secondary br2']"))).click()
        except:
            pass

        i+=1

    print(f'relevant jobs are {len(job_link)}')
    df = pd.DataFrame({"URL": job_link,'Job_Search':jobdesgn})
    return df
